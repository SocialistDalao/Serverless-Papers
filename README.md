# Function-as-a-Service (FaaS) & Serverless Computing papers

Make a pull request or issue to update your favourite papers here.



### Industrial View

[Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider | USENIX 2020](https://www.usenix.org/conference/atc20/presentation/shahrad) In this paper, we first characterize the entire production FaaS workload of Azure Functions. We show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from our characterization, we then propose a practical resource management policy that significantly reduces the number of function cold starts, while spending fewer resources than state-of-the-practice policies.

[XFaaS: Hyperscale and Low Cost Serverless Functions at Meta | Proceedings of the 29th Symposium on Operating Systems Principles (acm.org) 2023](https://dl.acm.org/doi/10.1145/3600006.3613155) to eliminate the cold start time of functions, XFaaS strives to approximate the effect that every worker can execute every function immediately. To handle load spikes without over-provisioning resources, XFaaS defers the execution of delay-tolerant functions to off-peak hours and globally dispatches function calls across datacenter regions. To prevent functions from overloading downstream services, XFaaS uses a TCP-like congestion-control mechanism to pace the execution of functions.

### Start Latency Optimization

[FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing | ACM Transactions on Software Engineering and Methodology 2023](https://dl.acm.org/doi/10.1145/3585007): Application code loading latency is a significant overhead. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure.

[Replayable Execution Optimized for Page Sharing for a Managed Runtime Environment | Proceedings of the Fourteenth EuroSys Conference 2019 (acm.org)](https://dl.acm.org/doi/10.1145/3302424.3303978): Replayable Execution exploits the intensive-deflated execution characteristics of the majority of target applications. It uses checkpointing to save an image of an application, allowing this image to be shared across containers and resulting in speedy restoration at service startup. 

[Help Rather Than Recycle: Alleviating Cold Startup in Serverless Computing Through Inter-Function Container Sharing | USENIX 2022](https://www.usenix.org/conference/atc22/presentation/li-zijun-help): We observe that some functions suffer from cold container startup, while the warm containers of other functions are idle. Based on the observation, other than booting a new container for a function from scratch, we propose to alleviate the cold startup by re-purposing a warm but idle container from another function.

[Catalyzer | Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (acm.org)](https://dl.acm.org/doi/10.1145/3373376.3378512) This paper proposes Catalyzer, a serverless sandbox system design providing both strong isolation and extremely fast function startup. Instead of booting from scratch, Catalyzer restores a virtualization-based function instance from a well-formed checkpoint image and thereby skips the initialization on the critical path (init-less). We also propose a new OS primitive, sfork (sandbox fork), to further reduce the startup latency by directly reusing the state of a running sandbox instance. Fundamentally, Catalyzer removes the initialization cost by reusing state, which enables general optimizations for diverse serverless functions.

[Fireworks | Proceedings of the Seventeenth European Conference on Computer Systems (acm.org) 2022](https://dl.acm.org/doi/10.1145/3492321.3519581) we propose a novel ***VM-level post-JIT snapshot\*** approach and develop a new serverless framework, ***Fireworks\*.** Our key idea is to synergistically leverage a virtual machine (VM)-level snapshot with a language runtime-level just-in-time (JIT) compilation in tandem. Fireworks leverages JITted serverless function code to reduce both start-up time and execution time of functions and improves memory efficiency by sharing the JITted code. Also, Fireworks can provide a high level of isolation by using a VM as a sandbox to execute a serverless function. 

[SOCK: Rapid Task Provisioning with Serverless-Optimized Containers | USENIX 2018](https://www.usenix.org/conference/atc18/presentation/oakes) In this work, we analyze Linux container primitives, identifying scalability bottlenecks related to storage and network isolation. We also analyze Python applications from GitHub and show that importing many popular libraries adds about 100ms to startup. Based on these findings, we implement SOCK, a container system optimized for serverless workloads. 

[SEUSS | Proceedings of the Fifteenth European Conference on Computer Systems (acm.org) 2020](https://dl.acm.org/doi/10.1145/3342195.3392698) This paper presents a system-level method for achieving the rapid deployment and high-density caching of serverless functions in a FaaS environment. For reduced start times, functions are deployed from unikernel snapshots, bypassing expensive initialization steps. To reduce the memory footprint of snapshots we apply page-level sharing across the entire software stack that is required to run a function.

[Pronghorn: Effective Checkpoint Orchestration for Serverless Hot-Starts | Proceedings of the Nineteenth European Conference on Computer Systems (acm.org) 2024](https://dl.acm.org/doi/10.1145/3627703.3629556) This paper proposes Pronghorn, a snapshot serverless orchestrator that automatically monitors the function performance and decides (1) when it is the right moment to take a snapshot and (2) which snapshot to use for new workers. Pronghorn is agnostic to the underlying platform and JIT runtime, thus easing its integration into existing runtimes and worker deployment environments (container, virtual machine, etc.).

[SAND: Towards High-Performance Serverless Computing | USENIX 2018](https://www.usenix.org/conference/atc18/presentation/akkus)  existing serverless platforms normally isolate and execute functions in separate containers, and do not exploit the interactions among functions for performance. These practices lead to high startup delays for function executions and inefficient resource usage. This paper presents SAND, a new serverless computing system that provides lower latency, better resource efficiency and more elasticity than existing serverless platforms. To achieve these properties, SAND introduces two key techniques: 1) application-level sandboxing, and 2) a hierarchical message bus. We have implemented and deployed a complete SAND system.

### Benchmark & Empirical Analysis

[Peeking Behind the Curtains of Serverless Platforms | USENIX ATC 18](https://www.usenix.org/conference/atc18/presentation/wang-liang): Taking on the viewpoint of a serverless customer, we conduct the largest measurement study to date, launching more than 50,000 function instances across these three services, in order to characterize their architectures, performance, and resource management efficiency. We explain how the platforms isolate the functions of different accounts, using either virtual machines or containers, which has important security implications. We characterize performance in terms of scalability, coldstart latency, and resource efficiency, with highlights including that AWS Lambda adopts a bin-packing-like strategy to maximize VM memory utilization, that severe contention between functions can arise in AWS and Azure, and that Google had bugs that allow customers to use resources for free.

[SeBS | Proceedings of the 22nd International Middleware Conference (acm.org) 2021](https://dl.acm.org/doi/abs/10.1145/3464298.3476133) we propose the Serverless Benchmark Suite: the first benchmark for FaaS computing that systematically covers a wide spectrum of cloud resources and applications. Our benchmark consists of the specification of representative workloads, the accompanying implementation and evaluation infrastructure, and the evaluation methodology that facilitates reproducibility and enables interpretability. We demonstrate that the abstract model of a FaaS execution environment ensures the applicability of our benchmark to multiple commercial providers such as AWS, Azure, and Google Cloud. Our work facilities experimental evaluation of serverless systems, and delivers a standardized, reliable and evolving evaluation methodology of performance, efficiency, scalability and reliability of middleware FaaS platforms.

[FaaSdom | Proceedings of the 14th ACM International Conference on Distributed and Event-based Systems 2020](https://dl.acm.org/doi/10.1145/3401025.3401738) FaaSdom is a modular architecture and proof-of-concept implementation of a benchmark suite for serverless computing platforms. It currently supports the current mainstream serverless cloud providers (*i.e.*, AWS, Azure, Google, IBM), a large set of benchmark tests and a variety of implementation languages. The suite fully automatizes the deployment, execution and clean-up of such tests, providing insights (including historical) on the performance observed by serverless applications. FaaSdom also integrates a model to estimate budget costs for deployments across the supported providers.

[Characterizing serverless platforms with serverlessbench | Proceedings of the 11th ACM Symposium on Cloud Computing 2020](https://dl.acm.org/doi/10.1145/3419111.3421280) This paper proposes ServerlessBench, an open-source benchmark suite for characterizing serverless platforms. It includes test cases exploring characteristic metrics of serverless computing, e.g., communication efficiency, startup latency, stateless overhead, and performance isolation. We have applied the benchmark suite to evaluate the most popular serverless computing platforms, including AWS Lambda, Open-Whisk, and Fn, and present new serverless implications from the study.



### Miscellaneous

- Communication & Network
  - [Nightcore: efficient and scalable serverless computing for latency-sensitive, interactive microservices | Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems 2021](https://dl.acm.org/doi/10.1145/3445814.3446701): However, current serverless platforms have millisecond-scale runtime overheads, making them unable to meet the strict sub-millisecond latency targets required by existing interactive microservices. We present Nightcore, a serverless function runtime with microsecond-scale overheads that provides container-based isolation between functions. Nightcoreâ€™s design carefully considers various factors having microsecond-scale overheads, including scheduling of function requests, communication primitives, threading models for I/O, and concurrent function executions. 
- Edge Computing
  - [Sledge | Proceedings of the 21st International Middleware Conference (acm.org) 2020 ](https://dl.acm.org/doi/10.1145/3423211.3425680) Despite a variety of existing commercial and open source serverless platforms (utilizing VMs and containers), these solutions are too heavy-weight for a resource-constrained Edge systems (due to large memory footprint and high invocation time). In this paper, we present the design and implementation of Sledge -- a novel and efficient WebAssembly-based serverless framework for the Edge. Sledge is designed for these constraints by offering (i) optimized scheduling policies and efficient work-distribution for short-lived computations, and (ii) a light-weight function isolation model implemented using our own WebAssembly-based software fault isolation infrastructure.
- Heterogenous Computing
  - [Serverless computing on heterogeneous computers | Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems 2022](https://dl.acm.org/doi/10.1145/3503222.3507732) We introduce Molecule, the first serverless computing system utilizing heterogeneous computers. @e first propose XPU-Shim, a distributed shim to bridge the gap between underlying multi-OS systems (when using general-purpose devices) and our serverless runtime (i.e., Molecule). We further introduce vectorized sandbox, a sandbox abstraction to abstract hardware heterogeneity (when using domain-specific accelerators). Moreover, we also review state-of-the-art serverless optimizations on startup and communication latency and overcome the challenges to implement them on heterogeneous computers.

### Survey 

[A Holistic View on Resource Management in Serverless Computing Environments: Taxonomy and Future Directions | ACM Computing Surveys](https://dl.acm.org/doi/full/10.1145/3510412): In this article, we identify the major aspects covering the broader concept of resource management in serverless environments and propose a taxonomy of elements that influence these aspects, encompassing characteristics of system design, workload attributes, and stakeholder expectations. We take a holistic view on serverless environments deployed across edge, fog, and cloud computing networks. We also analyse existing works discussing aspects of serverless resource management using this taxonomy. This article further identifies gaps in literature and highlights future research directions for improving capabilities of this computing model.

[The Serverless Computing Survey: A Technical Primer for Design Architecture | ACM Computing Surveys 2022](https://dl.acm.org/doi/full/10.1145/3508360): this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.

[Serverless computing: a security perspective | Journal of Cloud Computing | Full Text (springeropen.com) 2022](https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-022-00347-w): In this article we review the current serverless architectures, abstract and categorize their founding principles, and provide an in-depth security analysis. In particular, we: show the security shortcomings of the analyzed serverless architectural paradigms; point to possible countermeasures; and, highlight several research directions for practitioners, Industry, and Academia.

[[2310.08437\] Cold Start Latency in Serverless Computing: A Systematic Review, Taxonomy, and Future Directions (arxiv.org) 2023](https://arxiv.org/abs/2310.08437): To study the "cold start" problem in serverless computing, this article provides a comprehensive literature overview of recent research. In addition, we present a detailed taxonomy of several approaches to addressing the issue of cold start latency in serverless computing. There are several categories in which a current study on cold start latency is organised: caching and application-level optimization-based solutions, as well as AI/ML-based solutions. We have analysed the current methods and grouped them into categories based on their commonalities and features. 
